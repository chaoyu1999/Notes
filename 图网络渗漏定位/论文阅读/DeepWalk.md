# 1、DeepWalk: Online Learning of Social Representations，随机游走、图嵌入

## 图嵌入

图嵌入是一种将图数据（通常为高维稠密的矩阵）映射为低微稠密向量的过程,如图。图嵌入需要捕捉到图的拓扑结构，顶点与顶点的关系，以及其他的信息 （如子图，连边等）。如果有更多的信息被表示出来，那么下游的任务将会获得更好的表现。在嵌入的过程中存在着一种共识：向量空间中保持连接的节点彼此靠近。

**DeepWalk的思想：**

使用随机游走生成节点序列，将这个节点序列作为“句子”输入到Word2Vec模型中，得到每个节点的嵌入向量。

## 摘要

我们提出了Deep Walk，一种新的**学习网络中顶点潜在表示的方法**。这些潜在表示将社会关系编码在一个连续的向量空间中，很容易被统计模型所利用。DeepWalk概括了语言建模和从词序列到图的无监督特征学习(或深度学习)的最新进展。DeepWalk利用截断随机游走获得的局部信息，通过将游走视为句子的等价替换来学习潜在表示。

## 引言

网络表示的稀疏性既是优点也是弱点。稀疏性可以设计高效的离散算法，但在统计学习中难以泛化。

我们开发了一种算法（DeepWalk），通过建模一系列短随机游走，学习图的顶点的社交表示。社交表示是顶点的潜在特征，捕获邻域相似性和社区成员资格。这些潜在表示将社交关系编码到一个维数相对较小的连续向量空间中。

我们评估了其在**大型异构图中具有挑战性的多标签网络分类问题**上的性能。在关系分类问题中，**特征向量之间的联系违背了传统的独立同分布假设**。解决这个问题的技术通常使用近似推理技术[ 31、35]来利用依赖信息来改进分类结果。我们通过学习图的与标签无关的表示来远离这些方法。我们的表示质量不受标记顶点选择的影响，因此它们可以在任务间共享。

论文贡献：

- 我们引入深度学习作为分析图的工具，构建适用于统计建模的鲁棒表示。Deep Walk学习短随机游动中的结构规律。
- 我们在多个社交网络上的多标签分类任务上广泛评估了我们的表示。在标签稀疏性存在的情况下，我们展示了显著提高的分类性能，在我们考虑的最稀疏问题上，获得了Micro F1的5 % - 10 %的提升。在某些情况下，即使在给定60 %的训练数据的情况下，DeepWalk的表示也能超过其竞争对手。
- 我们通过构建网络规模图的表示，(例如YouTube)使用并行实现来展示我们算法的可扩展性。此外，我们描述了构建我们的方法的流版本所需的最小更改。

## 问题定义

我们考虑将社交网络的成员分类到一个或多个类别的问题。设 $$G=(V, E)$$，其中 $$V$$ 是网络的成员，$$E$$ 是其边，$$E\subseteq(V\times V)$$。给定一个部分标记的社交网络 $$G_L=(V,E,X,Y)$$，其中属性 $$X\in\mathbb{R}^{|V|\times S}$$，$$S$$ 是每个属性向量的特征空间的大小，以及 $$Y\in\mathbb{R}^{|V|\times|\mathcal{Y}|}$$，$$\mathcal{Y}$$ 是标签集。 

在传统的机器学习分类设置中，我们的目标是学习一个假设 $$H$$，将 $$X$$ 的元素映射到标签集 $$Y$$。在我们的情况下，我们可以利用嵌入在 $$G$$ 结构中的示例依赖性的重要信息来实现优越的性能。 

我们的目标是学习 $$X_E\in\mathbb{R}^{|V|\times d}$$，其中 $$d$$ 是潜在维度的小数量。这些低维表示是分布式的；意味着每个社交现象都由维度的一个子集来表达，每个维度都贡献于空间所表达的社交概念的一个子集。 

## 随机游走

给定一个图和一个起点，我们随机选择它的一个邻居，并移动到这个邻居；接着我们随机选择这一点的一个邻居，然后继续移动到它，重复这个操作。以这种方式访问的(随机)节点序列是图上的一个随机游走。

![](https://raw.githubusercontent.com/onlyfabin/PB_1/main/markdown/202401231357338.png)

我们将以顶点 $$v_i$$ 为根的随机游走记为 $$\mathcal{W}_{v_i}$$。它是一个随机过程，具有随机变量 $$W_{v_i}^1,W_{v_i}^2,\ldots,W_{v_i}^k$$，使得 $$\mathcal{W}_{v_i}^{k+1}$$ 是从顶点 $$v_k$$ 的邻居中随机选择的一个顶点。随机游走已经被用作各种问题的相似性度量，如内容推荐和社区检测。它们也是一类敏感于输出的算法的基础，这些算法使用它们在小于输入图大小的时间内计算局部社区结构信息。 

正是这种与局部结构的联系，激励我们使用一系列短随机游走作为从网络中提取信息的基本工具。除了捕获社区信息外，将随机游走作为我们算法的基础还给我们带来了其他两个理想的特性。首先，局部探索易于并行化。几个随机游走者（在不同的线程、进程或机器中）可以同时探索同一图的不同部分。其次，依赖从短随机游走中获得的信息，使得我们可以在不需要全局重新计算的情况下，适应图结构的小变化。我们可以用来自变化区域的新随机游走迭代地更新学习到的模型，其时间复杂度是小于整个图的。 

![](https://raw.githubusercontent.com/onlyfabin/PB_1/main/markdown/202401231132469.png)

外循环指定了在每个顶点开始随机游走的次数γ。我们认为每次迭代都是对数据进行一次"遍历法"，并在此遍历法中每个节点采样一次游走。在每次遍历的开始，我们生成一个随机的顺序来遍历顶点。这是没有严格要求的，但众所周知，它可以加快随机梯度下降的收敛速度。

在内层循环中，对图的所有顶点进行迭代。对于每个顶点$v_i$，我们生成一个随机游走$| W_{v_i} | = t$，然后用它来更新我们的表示。我们使用skipgram算法根据我们的目标函数在方程中更新这些表示。

![](https://raw.githubusercontent.com/onlyfabin/PB_1/main/markdown/202401231350718.png)

skip-gram模型是根据中心词预测上下文m个词的算法。它最大化了在句子中出现的窗口w内的单词间的共现概率[26]。

算法2遍历所有在随机漫步中出现在窗口w内的可能的搭配（第1-2行）。对于每一个，我们将每个顶点$v_j$映射到其当前的表示向量$\Phi(v_j)\in\mathbb{R}^d$（见图3b）。给定$v_j$的表示，我们希望最大化其在漫步中的邻居的概率（第3行）。我们可以使用几种选择的分类器来学习这样的后验分布。例如，用逻辑回归模型化前面的问题，会导致标签数量巨大，等于$|V|$，可能是百万或亿级。这样的模型需要大量的计算资源，可能需要一个计算机集群。为了加快训练时间，可以使用分级Softmax[29,30]来近似概率分布。